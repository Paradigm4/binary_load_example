# Quick binary load examples

This repository includes a simple but relatively fast binary data generator
program and example SciDB binary load scripts that illustrate sequential and
parallel binary data load. The load scripts also redimension the example data
into a 2-d array to simulate typical data load workflows.

You can run these tests on your own system. The timings reported here are
examples and were run on a single Amazon EC2 i2.8xlarge instance. As of March,
2016, such an instance includes 32 vCPUs, 8 SSD storage devices, and 250 GB
RAM.

All tests run SciDB in a RAM-based tmpfs file system using the configuration
setting:
```
base-path=/dev/shm
```
We conduct tests in tmpfs to minimize storage system artifacts and bandwidth
limitations; for instance, the Amazon i2.8xlarge AMI is only guaranteed to
have approximately 500 MB/s *overall* I/O rate to storage devices, but is
capable of much higher bandwidth to RAM-based file systems.


## Compiling the example data generator

The simple `generator.c` program produces a specified number of records and
writes them in binary format to stdout. Each record consists of 96 bytes as ten
64-bit double-precision numbers and 4 32-bit integers. The program simulates a
data frame or data table like binary output written row-wise.

```
# clone the project
git clone https://github.com/Paradigm4/binary_load_example.git
cd binary_load_example

# compile
gcc -O3 -o generator generator.c

# run with
./generator N start
```

The program requires two arguments, `N` is the number of 96-byte records to
write to stdout and `start` is the starting record index. The `start` argument
is there to allow multiple copies of the generator process to write the same
output that a single run can write.

## Reference testing

These tests show how fast a single generator process can run on this system.

### Single process data generation (no write): 2.7 GB/s
The first test shows that the generator can produce 10 million records
(960,000,000 bytes) in about 1/3 second on this system, or about 2.7
GBytes/second.

```
time ./generator 10000000 0 > /dev/null

real    0m0.350s
user    0m0.149s
sys     0m0.201s
```

### Single process data generation to RAM: 1.3 GB/s
The next test shows how fast a single generator process can write to a
RAM-based tmpfs file system, about 1.3 GB/s. This test establishes an
upper-bound for single-process write rate.
```
time ./generator 10000000 0 > /dev/shm/test.bin

real    0m0.754s
user    0m0.140s
sys     0m0.614s
```

### Multi process data generation to RAM: 10.6 GB/s
The `multi_reference.sh` script writes in parallel to a RAM-based
tmpfs file system using 32 generator processes, establishing a maximum upper
bound for write rate using the example data generator on this system. On the
Amazon EC2 i2.8xlarge instance type we measured about 10.6 GB/s.
```
rm -f /dev/shm/out.*
time ./multi_reference.sh 

real    0m0.090s
user    0m0.172s
sys     0m0.819s
```
(See the `multi_reference.sh` script for details. Be sure to delete files from
previous runs before testing to avoid file system metadata overhead.)


### Multi process data output to 8 SSD devices: 1.8 GB/s

Just for fun, we tested the upper bound of writing data from our generator in
parallel to the 8 available SSD devices. These tests have no bearing on the
SciDB tests below, they simply provide an example of the maximum throughput to
I/O on this Amazon EC2 instance. It's higher than the advertised 500 MB/s from
Amazon.
```
./multi_ssd_setup.sh
time ./multi_ssd.sh

real    0m0.519s
user    0m0.188s
sys     0m1.843s
```


# Loading data into SciDB

We use the SciDB binary `input` operator in sequential and parallel modes in
the tests below.

## Single process SciDB binary load

The `single_process_flat.sh` loads data generated by a single process into a
so-called 'flat' SciDB array (no data clustering).  The
`single_process_redim.sh` script loads data generated by a single generator
process and clusters the data using the SciDB `redimension` operator into an
array that uses two of the generated integers as coordinate axes and the
remaining 12 doubles and integers as attribute values, representing a very
typical load workflow.

### Single-process flat array load: 37 MB/s
```
./single_process_flat.sh
Query was executed successfully

real    0m25.913s
user    0m1.125s
sys     0m1.928s
```

### Single-process redimension array load: 36 MB/s
```
./single_process_redim.sh 
Query was executed successfully

real    0m26.439s
user    0m1.165s
sys     0m1.993s
```


## Parallel SciDB binary load

The `multi_process_flat.sh` loads data generated by a 32 generator processes in
parallel into a so-called 'flat' SciDB array (no data clustering).  The
`multi_process_redim.sh` script loads data generated by 32 generator processes
and clusters the data using the SciDB `redimension` operator into an array that
uses two of the generated integers as coordinate axes and the remaining 12
doubles and integers as attribute values, representing a very typical load
workflow.

### Parallel flat array load: 491 MB/s

```
 ./multi_process_flat.sh
Query was executed successfully

real    0m1.956s
user    0m0.501s
sys     0m0.657s
```

### Parallel redimension array load: 128 MB/s
```
./multi_process_redim.sh
Query was executed successfully

real    0m7.483s
user    0m0.295s
sys     0m0.716s
```
